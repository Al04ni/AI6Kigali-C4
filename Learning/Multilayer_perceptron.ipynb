{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training an MLP on MNIST with Numpy\n",
        "\n",
        "In this Notebook we will implement and train Multi-Layer Perceptron on the MNIST dataset."
      ],
      "metadata": {
        "id": "VpZtoPcE5pmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and prepare the Dataset"
      ],
      "metadata": {
        "id": "Qbp7QMLL6Oz_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aju3HXLx5cd_",
        "outputId": "945f9c0e-a4cb-421b-8153-f605acc8d6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y = True)\n",
        "X = X.values # To return the values into array\n",
        "y = y.astype(int).values"
      ],
      "metadata": {
        "id": "ZYT3ZHVt6aHx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the data\n",
        "X = ((X / 255.0) - .5) * 2 # Here we rescale the pixel values to train more effectively"
      ],
      "metadata": {
        "id": "tXrZ6Ty_7AZ7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize our sample digits\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
        "\n",
        "ax = ax.flatten()\n",
        "for i in range(10):\n",
        "    img = X[y == i][0].reshape(28, 28)\n",
        "    ax[i].imshow(img, cmap='Greys')\n",
        "ax[0].set_xticks([])\n",
        "ax[0].set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "tzvsZByX7Ott",
        "outputId": "83757a09-876f-4611-8344-438535f891a0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Validation-Test split"
      ],
      "metadata": {
        "id": "Le_p-mRx78PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=123)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=55000, random_state=123)"
      ],
      "metadata": {
        "id": "JEgkQHGi7lWi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_val.shape, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCo6US22_1FV",
        "outputId": "3e015c97-8d7a-4f30-c263-16753b7d721f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((55000, 784), (5000, 784), (10000, 784))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape, y_val.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhEzSNOfATni",
        "outputId": "651fa1b2-e9c9-45b6-e50e-f6d0101d1fe1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((55000,), (5000,), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Mini - batches"
      ],
      "metadata": {
        "id": "JCVmwdeA-T0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-sjXDxq-_LZ",
        "outputId": "39c98451-f3e0-43bd-8038-da1d2ee0c0d1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def minibatch_generator(X, y, minibatch_size):\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    for start_idx in range(0, indices.shape[0] - minibatch_size + 1, minibatch_size):\n",
        "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
        "        yield X[batch_idx], y[batch_idx]"
      ],
      "metadata": {
        "id": "KGmgitda-cX6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core componets of MLP"
      ],
      "metadata": {
        "id": "_Th-xcVsBBhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Layer"
      ],
      "metadata": {
        "id": "ondGG3A3BctF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "    def __init__(self, in_features, out_features):\n",
        "        # TODO\n",
        "        self.weight = np.random.rand(in_features, out_features)  # Initialize the weight randomly.\n",
        "        self.bias = np.zeros(out_features) # Initialize the bias to all zeros.\n",
        "\n",
        "        self.grad_weight = np.zeros((in_features, out_features)) # Initialize the weight gradient to zeros. The shape should same as self.weight\n",
        "        self.grad_bias = np.zeros(out_features) # Initialize the bias gradient to zeros. The shape should same as self.bias\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.input = x # Do not remove. Needed for backward computation\n",
        "\n",
        "        # The general Linear equation\n",
        "        output = np.dot(x, self.weight) + self.bias # TODO: Compute the net input based in the linear equation.\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        grad_input = np.dot(grad_output, self.weight.T)\n",
        "\n",
        "        self.grad_weight[...] = np.dot(self.input.T, grad_output)\n",
        "        self.grad_bias[...] = np.mean(grad_output, axis=0)\n",
        "        return grad_input\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"Returns the parameters of your linear layer\"\"\"\n",
        "        return [self.weight, self.bias]\n",
        "\n",
        "    def gradients(self):\n",
        "        \"\"\"Returns the gradients for the parameters of your linear layer\"\"\"\n",
        "        return [self.grad_weight, self.grad_bias]"
      ],
      "metadata": {
        "id": "JigcHDre_H7K"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sigmoid Activation Function"
      ],
      "metadata": {
        "id": "tl8FfgXoE3fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.output = 1 /(1 + np.exp(x)) # TODO: Compute the sigmoid equation\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        return grad_output * self.output * (1 - self.output)"
      ],
      "metadata": {
        "id": "gLTfqUjJC6hD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean Squared Error Loss"
      ],
      "metadata": {
        "id": "xTknGyJUG2nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MSELoss:\n",
        "    def __call__(self, pred, target):\n",
        "        return self.forward(pred, target)\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        self.pred = pred # Do not remove. Needed for backward computation\n",
        "        self.target = target # Do not remove. Needed for backward computation\n",
        "        error = np.mean((self.pred - self.target)**2) # TODO: Compute the mean square error function\n",
        "        return error\n",
        "\n",
        "    def backward(self):\n",
        "        return 2. * (self.pred - self.target) / self.target.shape[0]"
      ],
      "metadata": {
        "id": "kKJGl_M9Gx5R"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent Optimizer"
      ],
      "metadata": {
        "id": "knZ4FDH0Jyip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, params, grads, lr=0.1):\n",
        "        self.params = params\n",
        "        self.grads = grads\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Update the parameters (weight and bias) of your netork using the Gradient Descent algorithm.\"\"\"\n",
        "        for p, g in zip(self.params, self.grads):\n",
        "            p -= self.lr * g # gradient descent\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"Zeros out the gradient of your network\"\"\"\n",
        "        for g in self.grads:\n",
        "            g.fill(0.0)"
      ],
      "metadata": {
        "id": "wZzvA7ZAJKG5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assembling the MLP"
      ],
      "metadata": {
        "id": "oZ5iQiFlKcK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    def __init__(self, in_features, hidden_size, out_features):\n",
        "        # TODO Initialize your network\n",
        "\n",
        "        # Do not change this variable names. The are needed for backward computation\n",
        "        self.linear1 = Linear(in_features, hidden_size)\n",
        "        self.act1 = Sigmoid()\n",
        "        self.linear2 = Linear(hidden_size, out_features)\n",
        "        self.act2 = Sigmoid()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward propagation through your network\"\"\"\n",
        "        # TODO Pass x (input) to your network and get y (output)\n",
        "        z1 = self.linear1(x) # For the net input of the hidden layer\n",
        "        a1 = self.act1(z1) # Activation for the hidden layer\n",
        "        z2 = self.linear2(a1)\n",
        "        y = self.act2(z2) # Activation of the hidden layer\n",
        "        return y\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"Backpropagation through your network\"\"\"\n",
        "        grad_z2 = self.act2.backward(grad_output)\n",
        "        grad_a1 = self.linear2.backward(grad_z2)\n",
        "\n",
        "        grad_z1 = self.act1.backward(grad_a1)\n",
        "        _ = self.linear1.backward(grad_z1)\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"Return the parameters of your entire model/network\"\"\"\n",
        "        return self.linear1.parameters() + self.linear2.parameters()\n",
        "\n",
        "    def gradients(self):\n",
        "        \"\"\"Return the gradients of your entire model/network\"\"\"\n",
        "        return self.linear1.gradients() + self.linear2.gradients()"
      ],
      "metadata": {
        "id": "lDvGNaGeKYvi"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "MQzJ5AFFPuN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the integer laberl to one-hot vectors\n",
        "# And we use `One-Hot encoding` for support comparison scope\n",
        "def int_to_onehot(y, num_labels=10):\n",
        "      return np.eye(num_labels)[y]\n",
        "\n",
        "loss_fn = MSELoss() # Add the loss function"
      ],
      "metadata": {
        "id": "YBAG70rMQQwP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion):\n",
        "    total_loss = 0.0\n",
        "    num_samples = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = model(X_batch)\n",
        "        loss = criterion(preds, int_to_onehot(y_batch))\n",
        "\n",
        "        grad_output = loss_fn.backward() # Gradient of the loss w.r.t output\n",
        "\n",
        "        model.backward(grad_output)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss\n",
        "        num_samples += len(y_batch)\n",
        "\n",
        "    return total_loss / num_samples"
      ],
      "metadata": {
        "id": "TvNu2zDrL_P6"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes the loss and accuracy on the validation set\n",
        "\n",
        "def valid(model, valid_loader, criterion):\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for X_batch, y_batch in valid_loader:\n",
        "        preds = model(X_batch)\n",
        "        loss = criterion(preds, int_to_onehot(y_batch))\n",
        "\n",
        "        total_loss += loss\n",
        "\n",
        "        predicted_labels = np.argmax(preds, axis=1)\n",
        "        correct_predictions += (predicted_labels == y_batch).sum()\n",
        "        total_predictions += len(y_batch)\n",
        "\n",
        "    total_loss /= total_predictions\n",
        "    accuracy = (correct_predictions / total_predictions)*100\n",
        "    return total_loss, accuracy"
      ],
      "metadata": {
        "id": "1Wni0kH2Re2g"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate Model, Loss function and Optimizer"
      ],
      "metadata": {
        "id": "fJ02EHgxSaZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_features = 28 * 28      # Each image is 28x28 pixels\n",
        "hidden_size = 50           # Size of the hidden layer (can experiment)\n",
        "out_features = 10          # One output per digit class (0-9)\n",
        "learning_rate = 0.1        # Step size for SGD updates\n",
        "\n",
        "np.random.seed(123)        # For reproducible results\n",
        "\n",
        "model = MLP(in_features=in_features, hidden_size=hidden_size, out_features=out_features)\n",
        "\n",
        "loss_fn = MSELoss()\n",
        "\n",
        "optimizer = SGD(model.parameters(), model.gradients(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "yEU8sAMMRttN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "b0mCjfpmTGvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 70\n",
        "batch_size = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Usually in the PyTorch way, dataloader SHOULD NOT be in the epoch loop\n",
        "    train_loader = minibatch_generator(X_train, y_train, minibatch_size=batch_size)\n",
        "    valid_loader = minibatch_generator(X_val, y_val, minibatch_size=batch_size)\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion=loss_fn)\n",
        "    valid_loss, valid_acc = valid(model, valid_loader, criterion=loss_fn)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1:02d}/{num_epochs} | Train MSE: {train_loss:.4f} | Valid MSE: {valid_loss:.4f} | Valid Acc: {valid_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHcGJOn_SucZ",
        "outputId": "7af0bf75-fa8b-4482-9a53-c8750dbdfa20"
      },
      "execution_count": 41,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 02/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 03/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 04/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 05/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 06/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 07/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 08/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 09/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 10/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 11/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 12/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 13/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 14/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 15/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 16/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 17/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 18/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 19/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 20/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 21/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 22/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 23/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 24/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 25/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 26/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 27/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 28/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 29/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 30/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 31/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 32/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 33/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 34/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 35/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 36/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 37/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 38/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 39/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 40/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 41/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 42/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 43/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 44/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 45/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 46/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 47/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 48/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 49/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 50/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 51/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 52/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 53/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 54/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 55/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 56/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 57/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 58/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 59/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 60/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 61/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 62/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 63/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 64/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 65/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 66/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 67/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 68/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 69/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n",
            "Epoch: 70/70 | Train MSE: 0.0010 | Valid MSE: 0.0010 | Valid Acc: 9.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure the final generalization\n",
        "def test(model, test_loader):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        preds = model(X_batch)\n",
        "\n",
        "        predicted_labels = np.argmax(preds, axis=1)\n",
        "        correct_predictions += (predicted_labels == y_batch).sum()\n",
        "        total_predictions += len(y_batch)\n",
        "\n",
        "    accuracy = (correct_predictions / total_predictions)*100\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "test_loader = minibatch_generator(X_test, y_test, minibatch_size=100)\n",
        "test_acc = test(model, test_loader)\n",
        "print(f'Test accuracy: {test_acc:.2f}%')#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWxJHsLfT1VM",
        "outputId": "bd8b3f25-1a76-4fb9-ef15-94207b89886f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 9.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lxS_tqcRUiJQ"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}